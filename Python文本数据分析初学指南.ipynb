{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jieba介绍](http://www.oschina.net/p/jieba/?fromerr=9NIIpUbk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[了解jieba](https://datartisan.gitbooks.io/begining-text-mining-with-python/content/%E7%AC%AC4%E7%AB%A0%20%E5%88%86%E8%AF%8D%E4%B8%8E%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/4.1%20%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%8F%8A%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "喜欢\n",
      "上海\n",
      "上海东方\n",
      "海东\n",
      "东方\n",
      "东方明珠\n",
      "方明\n",
      "明珠\n"
     ]
    }
   ],
   "source": [
    "s1 = \"我喜欢上海东方明珠\"\n",
    "w1 = jieba.cut(s1,cut_all = True)\n",
    "for i in w1:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我,来到,北京,清华,清华大学,华大,大学\n"
     ]
    }
   ],
   "source": [
    "s2 = \"我来到北京清华大学\"\n",
    "w2 = jieba.cut(s2,cut_all = True)\n",
    "print (','.join(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精准模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图将句子最精确地切开，适合文本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我,喜欢,上海,东方明珠\n"
     ]
    }
   ],
   "source": [
    "w3 = jieba.cut(s1)\n",
    "print (','.join(w3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我,来到,北京,清华大学\n"
     ]
    }
   ],
   "source": [
    "w4 = jieba.cut(s2)\n",
    "print(','.join(w4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搜索引擎模式cut_for_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我,喜欢,上海,东方,方明,明珠,东方明珠\n"
     ]
    }
   ],
   "source": [
    "w5 = jieba.cut_for_search(s1)\n",
    "print (','.join(w5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明,硕士,毕业,于,中国,科学,学院,科学院,中国科学院,计算,计算所,，,后,在,日本,京都,大学,日本京都大学,深造\n"
     ]
    }
   ],
   "source": [
    "s3 = \"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\"\n",
    "w6 = jieba.cut_for_search(s3)\n",
    "print (','.join(w6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注posseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "a:形容词\n",
    "c：连词\n",
    "d：副词\n",
    "e：叹词\n",
    "f：方位词\n",
    "i：成语\n",
    "m：数词\n",
    "n：名词\n",
    "nr：人名\n",
    "ns：地名\n",
    "nt：机构团体\n",
    "nz：其他专有名词\n",
    "p：介词\n",
    "r：代词\n",
    "t：时间\n",
    "u：助词\n",
    "v：动词\n",
    "vn：动名词\n",
    "w：标点符号\n",
    "un：未知词语\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "来到 v\n",
      "北京 ns\n",
      "清华大学 nt\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "w7 = pseg.cut(s2)\n",
    "for w in w7:\n",
    "    print (w.word,w.flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加自定义词典 jieba.load_userdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开发者可以指定自己自定义的词典，以便包含jieba词库里没有的词。虽然jieba有新词识别能力，但是自行添加新词可以保证更高的正确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "用法： jieba.load_userdict(file_name) file_name为自定义词典的路径\n",
    "\n",
    "词典格式和dict.txt一样，一个词占一行；每一行分三部分，一部分为词语，另一部分为词频，最后为词性（可省略），用空格隔开\n",
    "\n",
    "之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /\n",
    "\n",
    "加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict('userdict.txt')\n",
    "jieba.add_word('石墨烯')\n",
    "jieba.add_word('凱特琳')\n",
    "jieba.del_word('自定义词')\n",
    "test_sent = (\n",
    "\"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\"\n",
    "\"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\n",
    "\"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福/是/创新办/主任/也/是/云计算/方面/的/专家/;/ /什么/是/八一双鹿/\n",
      "/例如/我/输入/一个/带/“/韩玉赏鉴/”/的/标题/，/在/自定义词/库中/也/增加/了/此/词为/N/类/\n",
      "/「/台中/」/正確/應該/不會/被/切開/。/mac/上/可/分出/「/石墨烯/」/；/此時/又/可以/分出/來/凱特琳/了/。\n"
     ]
    }
   ],
   "source": [
    "words = jieba.cut(test_sent)\n",
    "print ('/'.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福 / nt , 是 / nt , 创新办 / nt , 主任 / nt , 也 / nt , 是 / nt , 云计算 / nt , 方面 / nt , 的 / nt , 专家 / nt , ; / nt ,   / nt , 什么 / nt , 是 / nt , 八一双鹿 / nt , \n",
      " / nt , 例如 / nt , 我 / nt , 输入 / nt , 一个 / nt , 带 / nt , “ / nt , 韩玉赏鉴 / nt , ” / nt , 的 / nt , 标题 / nt , ， / nt , 在 / nt , 自定义词 / nt , 库中 / nt , 也 / nt , 增加 / nt , 了 / nt , 此 / nt , 词 / nt , 为 / nt , N / nt , 类 / nt , \n",
      " / nt , 「 / nt , 台中 / nt , 」 / nt , 正確 / nt , 應該 / nt , 不 / nt , 會 / nt , 被 / nt , 切開 / nt , 。 / nt , mac / nt , 上 / nt , 可 / nt , 分出 / nt , 「 / nt , 石墨烯 / nt , 」 / nt , ； / nt , 此時 / nt , 又 / nt , 可以 / nt , 分出 / nt , 來 / nt , 凱特琳 / nt , 了 / nt , 。 / nt , "
     ]
    }
   ],
   "source": [
    "result = pseg.cut(test_sent)\n",
    "for r in result:\n",
    "    print (r.word,\"/\",w.flag,\",\",end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_install/ /is/ /great/.\n"
     ]
    }
   ],
   "source": [
    "terms = jieba.cut(\"easy_install is great.\")\n",
    "print (\"/\".join(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python/ /的/正则表达式/是/好用/的\n"
     ]
    }
   ],
   "source": [
    "terms = jieba.cut('python 的正则表达式是好用的')\n",
    "print('/'.join(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按标点分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text minming with python',\n",
       " 'TEXT MINING WITH PYTHON',\n",
       " 'Text Mining',\n",
       " 'With Python',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_split(sentence):\n",
    "    text_sen = []\n",
    "    for s in sentence.split('.'):\n",
    "        if '?' in s:\n",
    "            text_sen.extend(s.split('?'))\n",
    "        elif ',' in s:\n",
    "            text_sen.extend(s.split(','))\n",
    "        else:\n",
    "            text_sen.append(s)\n",
    "    return text_sen\n",
    "text=\"text minming with python,TEXT MINING WITH PYTHON.Text Mining?With Python.\"\n",
    "sentence_split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按空格分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['text', 'minming', 'with', 'python'],\n",
       " ['TEXT', 'MINING', 'WITH', 'PYTHON'],\n",
       " ['Text', 'Mining'],\n",
       " ['With', 'Python']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_word = []\n",
    "for i in sentence_split(text):\n",
    "    if i !=\"\":\n",
    "        text_word.append(i.split(\" \"))\n",
    "text_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sent_tokenize分句函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sent_tokenize 为 tokenize 分词包中的分句函数，返回文本的分句结果，调用方式为：sent_tokenize(text, language='english')\n",
    "\n",
    "参数说明：\n",
    "\n",
    "text:需要分句处理的文本\n",
    "\n",
    "language:nltk.tokenize.punkt 中包含了很多预先训练好的分词模型，参数 language 即为模型的名称\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me two of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\nThanks.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokenize分词函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "word_tokenize 为 tokenize 分词包中的分词函数，返回文本的分词结果，调用方式为：word_tokenize(text, language='english')\n",
    "\n",
    "参数说明：\n",
    "\n",
    "text: 需要分词处理的文本\n",
    "\n",
    "language: nltk.tokenize.punkt 中包含了很多预先训练好的分词模型，参数 language 即为模型的名称\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
       " ['Please', 'buy', 'me', 'two', 'of', 'them', '.'],\n",
       " ['Thanks', '.']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(t) for t in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regexp模块：正则表达式分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "对于包含比较复杂词型（如 $10、10%）的字符串，以上的分词算法往往不能实现精确分割，此时需要借助正则表达式来完成分词任务。\n",
    "NLTK 提供了 regexp 模块支持正则表达式分词，其中包括 regexp_tokenize 、wordpunct_tokenize、blankline_tokenize 等正则分词函数。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpTokenizer 类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "RegexpTokenizer 是 regexp 模块下一个类，可以自行定义正则表达式进行分词，调用该类下的分词方法需要先实例化，实例化方式为：实例=RegexpTokenizer(pattern, gaps, discard_empty, flags)\n",
    "\n",
    "参数说明：\n",
    "\n",
    "pattern：必填参数，构建分词器的模式，即正则表达式字符串\n",
    "\n",
    "gaps：可选参数，设置为 True 时，正则表达式指定识别标识符之间的间隔，默认缺失值为 False，即正则表达式用来识别标识符本身\n",
    "\n",
    "discard_empty：可选参数，设置为 True 时，去除任何由分词器产生的空符“\"”，只有当参数“gaps”取值为“True”时分词器才会差生空符\n",
    "\n",
    "flags：可选参数，编译分词器模式的正则标识，默认使用的是 re.UNICODE | re.MULTILINE | re.DOTALL\n",
    "\n",
    "实例化后即可利用该类下的分词方法进行分词处理，分词方法调用方式为：实例.tokenize（text）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/four/-poster/canopy/bed/made/in/U/.S.A./costs/$600./The/seller/stake/out/40/%/of/the/profit/.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"The four-poster canopy bed made in U.S.A. costs $600. The seller stake out 40% of the profit.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\"/\".join(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "也可以直接调用函数 regexp_tokenize 实现同样的分词效果，调用方式为：regexp_tokenize(text, pattern, gaps=False, discard_empty=True, flags=56)\n",
    "\n",
    "参数说明：\n",
    "\n",
    "text：必填参数，需要分词的字符串\n",
    "\n",
    "pattern：必填参数，构建分词器的模式，即正则表达式字符串\n",
    "\n",
    "gaps：可选参数，设置为 True 时，正则表达式指定识别标识符之间的间隔，默认缺失值为 False，即正则表达式用来识别标识符本身\n",
    "\n",
    "discard_empty：可选参数，设置为 True 时，去除任何由分词器产生的空符“\"”，只有当参数“gaps”取值为“True”时分词器才会差生空符\n",
    "\n",
    "flags：可选参数，编译分词器模式的正则标识，默认使用的是 re.UNICODE | re.MULTILINE | re.DOTALL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/four-poster/canopy/bed/made/in/U.S.A./costs/$/600/./The/seller/stake/out/40/%/of/the/profit/.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\"/\".join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = r\"\"\"(?x)                   # 设置以编写较长的正则条件\n",
    "              (?:[A-Z]\\.)+           # 缩略词 \n",
    "              |\\$?\\d+(?:\\.\\d+)?%?    # 货币、百分数\n",
    "              |\\w+(?:[-']\\w+)*       # 用连字符链接的词汇\n",
    "              |\\.\\.\\.                # 省略符号 \n",
    "              |(?:[.,;\"'?():-_`])    # 特殊含义字符 \n",
    "          \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/four-poster/canopy/bed/made/in/U.S.A./costs/$600/./The/seller/stake/out/40%/of/the/profit/.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/\".join(regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer按照空格分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WhitespaceTokenizer 子类，可以直接将字符串按照空格（包括space, tab, newline)分词，效果相当于利用字符串 split 方法。调用方式为：实例.tokenize（text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The four-poster canopy bed made in U.S.A. costs $600.\n",
      "The seller stake out 40% of the profit.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "text=\"The four-poster canopy bed made in U.S.A. costs $600.\\nThe seller stake out 40% of the profit.\"\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/four-poster/canopy/bed/made/in/U.S.A./costs/$600./The/seller/stake/out/40%/of/the/profit.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_space = WhitespaceTokenizer()\n",
    "\"/\".join(tokenizer_space.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer切分字母和非字母"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WordPunctTokenizer 子类，用正则表达式 “`\\w+|\\w\\s+” 将字符串切分成字母和非字母字符，分词方法调用方式为：实例.tokenize（text）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/four/-/poster/canopy/bed/made/in/U/./S/./A/./costs/$/600/./The/seller/stake/out/40/%/of/the/profit/.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer_punct = WordPunctTokenizer()\n",
    "\"/\".join(tokenizer_punct.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接调用函数 wordpunct_tokenize 实现同样的分词效果，调用方式为：wordpunct_tokenize（text）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/four/-/poster/canopy/bed/made/in/U/./S/./A/./costs/$/600/./The/seller/stake/out/40/%/of/the/profit/.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\"/\".join(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlanklineTokenizer将空行作为分隔符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "BlanklineTokenizer 子类， 将空行作为分隔符进行分词，空行是指不包含任何字符的行，空格 space 和制表符 tab 除外，相应的正则表达式为：'\\s\\n\\s\\n\\s*'。分词方法调用方式为：实例.tokenize（text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The four-poster canopy bed made in U.S.A. costs $600.\n",
      "\n",
      "The seller stake out 40% of the profit.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "text=\"The four-poster canopy bed made in U.S.A. costs $600.\\n\\nThe seller stake out 40% of the profit.\" \n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The four-poster canopy bed made in U.S.A. costs $600.',\n",
       " 'The seller stake out 40% of the profit.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_blank = BlanklineTokenizer()\n",
    "tokenizer_blank.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接调用函数 blankline_tokenize 实现同样的分词效果，调用方式为：blankline_tokenize（text）v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The four-poster canopy bed made in U.S.A. costs $600.',\n",
       " 'The seller stake out 40% of the profit.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "blankline_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stanford 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[在nltk中使用stanford](http://www.zmonster.me/2016/06/08/use-stanford-nlp-package-in-nltk.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tokenize 包中的 stanford 模块是 NLTK 的 Stanford Tokenizer 接口，模块中定义的 StanfordTokenizer 类提供了利用分词工具 PTBTokenizer 进行分词的方法，调用方式为：实例.tokenize(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import StanfordTokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "371px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
